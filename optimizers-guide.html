<!DOCTYPE html>
<html>
    <head>
        <title>How Adam works: Brief Guide to Optimizers</title>
        <link rel="stylesheet" type="text/css" href="/assets/style.css">
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <link rel="icon" href="/images/icon.png" type="image/x-icon">
        <style>
            body {
                font-family: sans-serif
            }
            p {
                margin-left: 100px;
                margin-right: 100px;
            }
            img {
                margin-left: 100px;
                margin-right: 100px;
            }
        </style>
    </head>

    <body>
        <div class="main-links">
            <a id="displayName" href="https://rayrwang.com">Raymond Wang</a>
            <br>
            <a href="https://rayrwang.com">Blog</a>
            <a href="https://rayrwang.com/about">About</a>
        </div>

        <p style="font-size: 30px; margin-bottom: 10px;"><strong>(not yet complete) How Adam works: Brief Guide to Optimizers</strong></p>
        
        <p><strong>Neural Network:</strong></p>
        <p>Here's your neural network:</p>
        <p>\(
            \begin{align*}
            &l = f(\boldsymbol{x}, \boldsymbol{\theta}) \\
            &l : loss \\
            &f : neural\ net\ architecture \\
            &\boldsymbol{x} : data\ vector\ (both\ inputs\ and\ optional\ labels) \\
            &\boldsymbol{\theta} : vector\ of\ parameters
            \end{align*}
            \)</p>
        <p><strong>Gradient:</strong></p>
        <p>And here's your gradient:</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{g} = \frac{dl}{d\boldsymbol{\theta}} \\
            &\boldsymbol{g} : gradient\ vector
            \end{align*}
            \)</p>
        <p><strong>Gradient descent:</strong></p>
        <p>Vanilla gradient descent:</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \boldsymbol{g} \\
            &\alpha : learning\ rate
            \end{align*}
            \)</p>
        <p><strong>Other things to add:</strong></p>
        <p><strong>Weight decay</strong></p>
        <p>Weight decay is a regularization technique that incentivises weights to be close to \(0\).
            This is done by adding the square of each weight to the loss.  
        </p>
        <p>New loss function:</p>
        <p>\(
            \begin{align*}
            &l = f(\boldsymbol{x}, \boldsymbol{\theta}) + \sum_i w_i^2 \\
            \end{align*}
            \)</p>
    </body>
</html>
