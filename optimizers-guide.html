<!DOCTYPE html>
<html>
    <head>
        <title>How Adam works: Brief Guide to Optimizers</title>
        <link rel="stylesheet" type="text/css" href="/assets/style.css">
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <link rel="icon" href="/images/icon.png" type="image/x-icon">
        <style>
            body {
                font-family: sans-serif
            }
            p {
                margin-left: 100px;
                margin-right: 100px;
            }
            img {
                margin-left: 100px;
                margin-right: 100px;
            }
        </style>
    </head>

    <body>
        <div class="main-links">
            <a id="displayName" href="https://rayrwang.com">Raymond Wang</a>
            <br>
            <a href="https://rayrwang.com">Blog</a>
            <a href="https://rayrwang.com/about">About</a>
        </div>

        <!-- <p style="font-size: 30px; margin-bottom: 10px;"><strong>(not yet complete) How Adam works: Brief Guide to Optimizers</strong></p> -->
        <h1 (not yet complete) How Adam works: Brief Guide to Optimizers</strong></p>

        <p><strong>Neural Network:</strong></p>
        <h2>Here's your neural network:</h2>
        <p>\(
            \begin{align*}
            &l = f(\boldsymbol{x}, \boldsymbol{\theta}) \\
            &l : loss \\
            &f : neural\ net\ architecture \\
            &\boldsymbol{x} : data\ vector\ (both\ inputs\ and\ optional\ labels) \\
            &\boldsymbol{\theta} : vector\ of\ parameters
            \end{align*}
            \)</p>
        <h2>Gradient:</h2>
        <p>And here's your gradient:</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{g} = \frac{dl}{d\boldsymbol{\theta}} \\
            &\boldsymbol{g} : gradient\ vector
            \end{align*}
            \)</p>
        <p><strong>Gradient descent:</strong></p>
        <p>Vanilla gradient descent. Parameters get updated using gradients to descend loss function.</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \boldsymbol{g} \\
            &\alpha : learning\ rate
            \end{align*}
            \)</p>
        <p><strong>Other things to add:</strong></p>
        <p>To improve on vanilla gradient descent, there are basically two things we can do.<br>
            1. Change the loss function.<br>
            2. Change how we use the gradients.
        </p>
        <p><strong>1. Change the loss function.</strong></p>
        <p><strong>Weight decay</strong></p>
        <p>Weight decay is a regularization technique that incentivises weights to be close to \(0\) to prevent overfitting.
            This is done by adding the square of each weight to the loss.  
        </p>
        <p>Old loss function:</p>
        <p>\(
            \begin{align*}
            &l = f(\boldsymbol{x}, \boldsymbol{\theta}) \\
            \end{align*}
            \)</p>
        <p>New loss function:</p>
        <p>\(
            \begin{align*}
            &l = f(\boldsymbol{x}, \boldsymbol{\theta}) + \lambda \sum_i \theta_i^2 \\
            &\lambda : (hyperparameter)\ weight\ decay\ strength
            \end{align*}
            \)</p>
        <p>New gradient:</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{g} = \frac{dl}{d\boldsymbol{\theta}} = \frac{d}{d \boldsymbol{\theta}} f(\boldsymbol{x}, \boldsymbol{\theta}) + 2 \lambda \boldsymbol{\theta} \\
            \end{align*}
            \)</p>
        <p>The factor of \(2\) gets absorbed into the hyperparameter \(\lambda\) for convenience.
        </p>
        <p><strong>2. Change how we use the gradients.</strong></p>
        <p><strong>Previous gradients</strong></p>
        <p>For the following, we will need to keep track of the gradient at previous timesteps.</p>
        <p>\(
            \begin{align*}
            &\boldsymbol{g_t} = Current\ gradient \\
            &\boldsymbol{g_{t-1}} = Gradient\ at\ previous\ timestep \\
            \end{align*}
            \)</p>
        <p>Momentum</p>

        <p><strong>Putting it all together: Adam</strong></p>
        <p><strong>AdamW</strong></p>
    </body>
</html>
